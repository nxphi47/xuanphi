#!/bin/env python2

import json, sys, pickle, os
import utils, collections
from copy import deepcopy

LABELS = ["C<15", "C>15"]

NTEST = 6
SMOOTH = 3
DIVIDER = 'split'
FEATURES = ['normTimeWidth', 'peakValue', 'totalArea', 'peakTime', '+gradRatio', '-gradRatio', '0gradRatio',
			'peakWidthDiff', 'peakWidthDiv', 'peakValueDiv', 'areaRatio', 'refTimeWidth-ave', 'refTimeWidth-immed', 
			'refTimeWidth-grad', 'gradChange' ]


def usage():
	print "Usage: %s <infile> <outfile> [nt=<num-test>] [sm=<smooth-int>] [dv=<dv_methods>]" % sys.argv[0]
	print "\tGenerate training samples values from the processed file containing all data\n"
	print "Parameters:"
	print "\t<infile>: the input file generated by process-json.py\n"
	print "\t<outdir | outfile>: the output dir to be generated or output file to be generate if user select feature"
	print "\t\tcombination manually. Take note that to indicate '.pkl' as file format\n"
	print "\t<num-test>: this is optional.  If specified as a non-zero number (say T), "
	print "\t\tevery T-th sample for each class is set aside as test sample. A file "
	print "\t\tnamed <outfile>.test will be created to store the test samples."
	print "\t\tDefault: 0\n"
	print "\t<sm=smooth-int>: this is optional.  If specified, it should be a number of sample to smooth."
	print "\t\tThis method implement for early binding on SVM will it will combine <smooth-int> number of sample's"
	print "\t\tfv together for SVM classification. It will generate a new training sample file with extension .seq"
	print "\t\tWhen perform svm-train, svm will check .seq file and load fv accordingly"
	print "\t\tDefault: %d\n" % SMOOTH
	print "\t<dv=dv_methods>: this is optional. If specified, it should be a string of how to identified trainnig run"
	print "\t\tand testing run."
	print "\t\t'split' - will seperate train and test json file"
	print "\t\t'smooth' - will seprate train and test sample based on order in processed file with SMOOTH integer"
	print "\t\t'side' - will discard first and last sample for each run and take only middle sample as test data"
	print "\t\tDefault: %s\n" % DIVIDER
	print
	exit(0)

# print feature selection and read input from user
def selection():
	print "\nSelect which feature going to extract:"
	print "\t1) If select multiple features, please separate feature number by space"
	print "\t2) If wish to select all features, key in 'all'\n"
	print "Features:"
	for ind, val in enumerate(FEATURES):
		print '\t%d %s' % (ind, val)
	print "\nKey in selection(s): "

# need to combine fv and label (float)  to make them a array
#	round each value to 2 decimal and return fv array
def get_extend_fv(sample, labels=LABELS):
	floats_label = []
	floats_fv = map(float, sample['fv'])
	floats_label.append(float(labels.index(sample['label'])))
	
	floats_fv.extend(floats_label) 
	fv = [round(float(i), 2) for i in floats_fv]
	return fv

# process for loop force fv file generation
#	infname	-	processed.data file path
#	outdir	- 	output directory of fv numpy file
#	feature_cnt	-	feature binary counter
#	resf	-	file to list out all fv and filename
# return None if error occur
def process_loop(infname, outdir, feature_cnt, resf):
	features = []
	f_str = ''
	for i,f in enumerate(FEATURES):
		if feature_cnt & (2**i) > 0:
			features.append(FEATURES[i])
			f_str = '1' + f_str
		else:
			f_str = '0' + f_str
	if len(features) < 2:
		# minimum 2 features
		return None
	
	print "\n****************************************************"
	print "Generated feature(%d:%s)=%s" % (feature_cnt, f_str, repr(features))
	print "***************************************************"
	
	fname = "fv-%d" % feature_cnt
	outfname = os.path.join(outdir, fname)
	pre_output_processing(infname, outfname, features, RESF=resf)

# process for single fv file generation with user selection of feature
#	infname	-	processed.data file path
#	outfname	- 	output file for fv numpy
#	response	- 	user input of feature selection, multiple selection split by space
def process_one(infname, outfname, response):
	features = []

	if 'all' in response:
		features = FEATURES
	else:
		_feature_input_split = response.split(" ")
		for ind, f in enumerate(FEATURES):
			if str(ind) in _feature_input_split:
				features.append(FEATURES[ind])
	
	print "\n****************************************************"
	print "Generated feature %s" % repr(features)
	print "***************************************************"
	
	pre_output_processing(infname, outfname.replace('.pkl', ''), features)

# pre-processing to generate neccessary files before go to generate sample
#	infname	-	processed.data file path
#	outfname	- 	output file for fv numpy
#	features	-	feature which use for generation
#	RESF	- optional, default to None. If user select loop force generation, this file will
#				list out all fv and filename
def pre_output_processing(infname, outfname, features, RESF=None):
	inf = open(infname, 'r')
	trainfname = outfname + '.samples'
	testfname =  outfname + '.samples.test'
	gen_training_sample(inf, trainfname, testfname, features)
	
	if not RESF is None:
		RESF.write("%s\t%s\n" % (os.path.basename(outfname), repr(features)))
	
	inf.close()

# output numpy using pickle
#	trainfname	-	training sample file name
#	train_list	-	list of training sample fv numpy
#	testfname	-	testing sample file name
#	test_list	-	list of testing sample fv numpy
def output_pickle(trainfname, train_list, testfname, test_list):
	with open(trainfname, 'wb') as f:
		pickle.dump(train_list, f)
	
	with open(testfname, 'wb') as f:
		pickle.dump(test_list, f)

# generate training and testing sample
#	inf	- processed.data file
#	trainfname	-	training sample file name
#	testfname	-	testing sample file name
#	features	- feature which use for generation
def gen_training_sample(inf, trainfname, testfname, features):
	labels={}
	dataID=[]
	datafv=[]
	lists=[]
	FILENAME=''
	trainl = []
	testl = []
	
	# if SMOOTH == 1, meaning that it not using SMOOTH function
	#	training sample will split by NTEST order where NTEST-th sample treat as test sample
	#	remaining as train sample
	if SMOOTH == 1:
		while True:
			sample = utils.get_sample(inf, features)
			if not sample:
				break
			if not sample.has_key('fv'):
				continue
			if not sample['label'] in labels:
				labels[sample['label']] = 0
			labels[sample['label']] += 1
			if labels[sample['label']] == NTEST:
				labels[sample['label']] = 0
				testl.append(get_extend_fv(sample))
			else:
				trainl.append(get_extend_fv(sample))
	else:
		# if SMOOTH in used, firstly combine fv based on SMOOTH sample
		while True:
			sample = utils.get_sample(inf, features)
			if not sample:
				break
			if not sample.has_key('fv'):
				continue
			if len(FILENAME) < 1 or not sample['filename'] == FILENAME:
				FILENAME = sample['filename']
				dataID = []
				datafv = []
			dataID.append(sample['id'])
			datafv.append(sample['fv'])
			if len(dataID) == SMOOTH:
				_dict={}
				_dict['id'] = dataID[0]
				_dict['joint-id'] = dataID
				_dict['filename'] = FILENAME
				_l = []
				for i in datafv:
					_l.extend(i)
				_dict['fv'] = _l
				#_dict['fv'] = datafv
				_dict['label'] = sample['label']
				#_odict = collections.OrderedDict(sorted(_dict.items()))
				lists.append(deepcopy(_dict))
				dataID.pop(0)
				datafv.pop(0)
		
		# IF DIVIDER == split, provide below steps:
		#	1. generate list of unique filename for each sample
		#	2. Divide list with 6 (5-to-1 train/test separation), X
		#	3. Divide X with number of different obstacle, Y
		#	4. Select Y file from end of list for each obstacle and insert to list, Z
		#	5. loop through sample list, if sample filename in Z, treat as test sample
		#	6. remain as training sample
		if DIVIDER == 'split':
			# get list of filename from list of dictionary
			_group = list(set([d['filename'] for d in lists if 'filename' in d]))
			
			import random
			_test_len = int(len(_group) / 6)
			#_test_json_file = random.sample(_group, _test_len)
			#_test_json_file = _group[len(_group)-_test_len:]
			_num = int(_test_len / 4)
			_remain = _test_len % _num
			_cub = _num + int(_remain / 2)
			
			_test_json_file = []
			l37, l20, l10, l7 = [], [], [], []
			for name in reversed(_group):
				if 'curb37.5' in name and len(l37) < _num:
						l37.append(name)
				if 'curb7.5' in name and len(l7) < _num:
					l7.append(name)
				if 'curb20' in name and not '-rev' in name and len(l20) < _cub:
					l20.append(name)
				if 'curb10' in name and not '-rev' in name and len(l10) < _cub:
					l10.append(name)
				if len(l37) == _num and len(l7) == _num and len(l20) == _cub and len(l10) == _cub:
					break
			_test_json_file = l7 + l10 + l20 + l37

			for l in lists:
				if l['filename'] in _test_json_file:
					testl.append(get_extend_fv(l))
				else:
					trainl.append(get_extend_fv(l))
		else:
			# if divider not split [smooth | side]
			if SMOOTH > 1:
				print 'SMOOTH > 1... ', SMOOTH
				# set how to split 
				if DIVIDER == 'smooth':
					_counter = 1
					for l in lists:
						if _counter < SMOOTH:
							trainl.append(get_extend_fv(l))
						else:
							testl.append(get_extend_fv(l))
							_counter = 0
						_counter += 1
				elif DIVIDER == 'side':
					# get list of filename from list of dictionary
					_group = [d['filename'] for d in lists if 'filename' in d]
					
					# get number of sample occur with same filename
					from collections import Counter
					c = Counter(_group)
					
					_counter = 0
					_filename = ''
					for l in lists:
						if len(FILENAME) < 1 or not l['filename'] == FILENAME:
							FILENAME = l['filename']
							_counter = 0
						_lnum = c[l['filename']]
						_num = int(_lnum / SMOOTH)
						# make sure always most middle sample choose
						if _lnum & 1 and _lnum > 4:
							_num += 1
						if not _counter == _num:
							trainl.append(get_extend_fv(l))
						else:
							testl.append(get_extend_fv(l))
						_counter += 1
	output_pickle(trainfname, trainl, testfname, testl)	
	
if __name__ == "__main__":
	if len(sys.argv) < 3:
		usage()
	
	# if load mode
	if 'load' in sys.argv[1]:
		with open(sys.argv[2], "rb") as handle:
			data = pickle.load(handle)
			print data
			exit(0)
	
	# reading for optional parameter change
	if len(sys.argv) > 3:
		for a in sys.argv[3:]:
			_buf = a.split('=')
			print _buf
			if _buf[0] == 'nt':
				NTEST = int(_buf[1])
			if _buf[0] == 'sm':
				SMOOTH = int(_buf[1])
			if _buf[0] == 'dv':
				DIVIDER = _buf[1]
	
	# check if file, that means user only want to output single file instead of all combination
	if '.pkl' in sys.argv[2]:
		_response = raw_input(selection())

		process_one(sys.argv[1], sys.argv[2], _response)
	else:	
		if not os.path.exists(sys.argv[2]):
			os.makedirs(sys.argv[2])
	
		resf = open(sys.argv[2] + "fv-list.txt", "w")
		if not resf:
			print "Error creating %s" % sys.argv[2]
			exit(2)
		
		for fc in xrange(3, 2**len(FEATURES)):
			process_loop(sys.argv[1], sys.argv[2], fc, resf) 
		
		resf.close()
	 
	
	
